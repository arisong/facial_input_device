# Introduction
Initially, I wanted to do something that we saw in *the* scene in 'Minority Report', but it did not seem practical because there was so much arm movements. It looked exhausting. In the end, I did this, which replaces major (not all) mouse functions with facial movements, so that we can, for example, browse the web just by using our face. The script reads your facial movements via your camera and imitate mouse actions.<br>
Transcription has also been added, so that there is no need for a keyboard to type words.<br>
Note that the UX is rather clunky now.

| Movements     | Actions       |
| ------------- | ------------- |
| Left eye blink  | Left click  |
| Right eye blink  | Right click  |
| Look up/down | Scroll up/down |
| Look up/down with mouth slightly open | Move mouse cursor up/down |
| Look right | Start transcription |
| Look left | (Placeholder) |

# Possible future additions
1. ~~Add speech transcription (so we can do away with the keyboard too)~~ DONE
2. Add voice commands e.g. to open Chrome.
3. Use eye gaze to point the mouse cursor instead.
4. Improve UX.

# References:
https://github.com/niconielsen32/ComputerVision<br>
https://github.com/ProgrammingHero1/eye_controlled_mouse<br>
https://realpython.com/python-speech-recognition/
